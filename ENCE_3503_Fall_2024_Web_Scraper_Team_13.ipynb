{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcsrLugRNvTM",
        "outputId": "65dcf568-f855-40bc-b1b2-1886ae929597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping pages 1 to 50 with 10 threads...\n",
            "Scraped data saved to ./data/scraped_data_team_13.csv. Total restaurants: 471.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Ensure the 'data' directory exists\n",
        "output_dir = \"./data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://oneclick.az/business/Restaurant/Restaurant/Restoranlar\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"}\n",
        "\n",
        "def scrape_page(page):\n",
        "    \"\"\"Scrape a single page.\"\"\"\n",
        "    url = f\"{BASE_URL}?city=784&page={page}\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        restaurants = []\n",
        "\n",
        "        # Adjust selectors based on your inspection\n",
        "        for item in soup.select(\".wrap\"):\n",
        "            # Extract name\n",
        "            h3 = item.select_one(\"h3\")\n",
        "            name = h3.select_one(\"a span\").text.strip() if h3 and h3.select_one(\"a span\") else \"No Name\"\n",
        "\n",
        "            # Extract address\n",
        "            address = item.select_one(\".f16\").text.strip() if item.select_one(\".f16\") else \"No Address\"\n",
        "\n",
        "            # Extract phone and site\n",
        "            ul = item.select_one(\"ul\")\n",
        "            phone = ul.select_one(\".phone\").text.strip() if ul and ul.select_one(\".phone\") else \"No Phone\"\n",
        "            site = ul.select_one(\".site a\")['href'].strip() if ul and ul.select_one(\".site a\") else \"No Site\"\n",
        "\n",
        "            # Skip this entry if the site URL is longer than 50 characters\n",
        "            if len(site) > 100:\n",
        "                continue\n",
        "\n",
        "            # Collect rating stars\n",
        "            star_container = item.select_one(\".right .star.hover\")\n",
        "            rating_stars = star_container.select_one(\"span[data-star]\")['data-star'] if star_container and star_container.select_one(\"span[data-star]\") else \"No Stars\"\n",
        "\n",
        "            # Extract votes dynamically for any star rating (0-5)\n",
        "            votes = \"No Votes\"\n",
        "            right = item.select_one(\".right\")\n",
        "            if right:\n",
        "                for star_num in range(6):  # Loop through star0 to star5\n",
        "                    star_class = f\".star.hover.star{star_num}\"\n",
        "                    star_element = right.select_one(star_class)\n",
        "                    if star_element:\n",
        "                        votes = star_element.text.strip()\n",
        "                        break  # Use the first matching vote count\n",
        "\n",
        "            # Extract restaurant type (Kind)\n",
        "            kind = right.select(\"a:nth-of-type(2)\")[0].text.strip() if right and len(right.select(\"a:nth-of-type(2)\")) > 0 else \"No Kind\"\n",
        "\n",
        "            # Split the 'Address' field into components\n",
        "            components = address.split(\",\")\n",
        "            city = components[0].strip() if len(components) > 0 else \"No City\"\n",
        "            district = components[1].strip() if len(components) > 1 else \"No District\"\n",
        "            street = components[2].strip() if len(components) > 2 else \"No Street\"\n",
        "            postal_address = components[3].strip() if len(components) > 3 else \"No Postal Address\"\n",
        "\n",
        "            restaurants.append({\n",
        "                \"Name\": name,\n",
        "                \"City\": city,\n",
        "                \"District\": district,\n",
        "                \"Street\": street,\n",
        "                \"Postal Address\": postal_address,\n",
        "                \"Phone\": phone,\n",
        "                \"Votes\": votes,\n",
        "                \"Rating Stars\": rating_stars,\n",
        "                \"Kind\": kind,\n",
        "                \"Site\": site\n",
        "            })\n",
        "\n",
        "        return restaurants\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping page {page}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_pages_in_parallel(start_page, end_page, max_threads=10):\n",
        "    \"\"\"Scrape multiple pages in parallel using ThreadPoolExecutor.\"\"\"\n",
        "    all_restaurants = []\n",
        "    with ThreadPoolExecutor(max_threads) as executor:\n",
        "        # Submit tasks to executor\n",
        "        futures = {executor.submit(scrape_page, page): page for page in range(start_page, end_page + 1)}\n",
        "\n",
        "        for future in futures:\n",
        "            page = futures[future]\n",
        "            try:\n",
        "                data = future.result()\n",
        "                if data:  # Only extend if valid data is returned\n",
        "                    all_restaurants.extend(data)\n",
        "                else:\n",
        "                    print(f\"No data returned for page {page}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing page {page}: {e}\")\n",
        "\n",
        "    return all_restaurants\n",
        "\n",
        "# Parameters\n",
        "start_page = 1\n",
        "end_page = 50  # Adjust the range as necessary\n",
        "max_threads = 10  # Number of threads\n",
        "\n",
        "# Scrape pages\n",
        "print(f\"Scraping pages {start_page} to {end_page} with {max_threads} threads...\")\n",
        "all_restaurants = scrape_pages_in_parallel(start_page, end_page, max_threads)\n",
        "\n",
        "# Save the data directly to a CSV file\n",
        "output_path = os.path.join(output_dir, \"scraped_data_team_13.csv\")\n",
        "columns = [\"Name\", \"City\", \"District\", \"Street\", \"Postal Address\", \"Phone\", \"Votes\", \"Rating Stars\", \"Kind\", \"Site\"]\n",
        "df = pd.DataFrame(all_restaurants, columns=columns)\n",
        "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Scraped data saved to {output_path}. Total restaurants: {len(df)}.\")\n"
      ]
    }
  ]
}
